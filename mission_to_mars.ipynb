{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_mars_news_url ='https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(nasa_mars_news_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are returned as an iterable list\n",
    "news_title  = soup.find('div', class_=\"content_title\").text  ###wl-troubleshoot - pulling wrong text\n",
    "\n",
    "news_p  = soup.find('div', class_=\"image_and_description_container\").text   ###wl-troubleshoot - pulling wrong text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL Mars Space Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "main_url='https://www.jpl.nasa.gov/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "JPL_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(JPL_url)\n",
    "browser.click_link_by_partial_text('FULL IMAGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load beautiful soup\n",
    "JPL_response = requests.get(JPL_url)\n",
    "JPL_soup = BeautifulSoup(JPL_response.text,'html.parser')\n",
    "#print(JPL_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get img src\n",
    "img_url = JPL_soup.find(\"img\",class_='thumb')\n",
    "#print(img_url) ###wl-comment: some reason this works\n",
    "#print(img_url['src']) ###wl-comment: some reason this works\n",
    "featured_image_url = main_url+img_url['src']\n",
    "#print(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###wl- bug: I'm getting the wrong image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser after scraping\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_url ='https://twitter.com/marswxreport?lang=en'\n",
    "#load beautiful soup\n",
    "weather_response = requests.get(weather_url)\n",
    "weather_soup = BeautifulSoup(weather_response.text,'html.parser')\n",
    "#print(weather_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tweet text\n",
    "weather_text = weather_soup.find_all(\"div\",class_=\"js-tweet-text-container\")\n",
    "#print(weather_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InSight sol 261 (2019-08-21) low -102.4ºC (-152.4ºF) high -26.6ºC (-15.8ºF)\n",
      "winds from the SSE at 4.9 m/s (11.0 mph) gusting to 16.0 m/s (35.8 mph)\n",
      "pressure at 7.70 hPa \n",
      "9:51 AM - 22 Aug 2019\n"
     ]
    }
   ],
   "source": [
    "text = weather_text[0]\n",
    "p = text.find('p').text\n",
    "#print(p)\n",
    "remove_link = p.split('pic')\n",
    "weather = remove_link[0]\n",
    "time_stamp = weather_soup.find(\"a\",class_=\"tweet-timestamp\")\n",
    "#print(time_stamp['title'])\n",
    "\n",
    "###wl - final ###\n",
    "mars_weather = str(weather) + \" \" + '\\n' + str(time_stamp['title'])\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for text in weather_text:           #tweet-timestamp\n",
    "#    p = text.find('p').text\n",
    "#    print(p)\n",
    "#    print('-'*55)\n",
    "#    time_stamp = weather_soup.find(\"a\",class_=\"tweet-timestamp\")\n",
    "#    print(time_stamp['title'])\n",
    "    ###wl - remove link\n",
    "    ###wl - combine strings\n",
    "    ###wl - find latest tweet\n",
    "    ###wl - remove \"InSight\"\n",
    "    ###wl - select one\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wl030\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\wl030\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.BNVRK7633HSX7YVO2TADGR4A5KEKXJAW.gfortran-win_amd64.dll\n",
      "C:\\Users\\wl030\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.CSRRD7HKRKC3T3YXA7VY7TAZGLSWDKW6.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Mars Facts website and use pandas to read the tables.  The table we are interested in is the first one.  \n",
    "#Save the dataframe to an table.html file.\n",
    "facts_url='https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(facts_url)\n",
    "facts_df = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_df.to_html('table.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###wl- bug - table shows weird characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "hemispheres_url='https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "hemispheres_main='https://astrogeology.usgs.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load beautiful soup\n",
    "hemispheres_response = requests.get(hemispheres_url)\n",
    "hemispheres_soup = BeautifulSoup(hemispheres_response.text,'html.parser')\n",
    "#print(hemispheres_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the 4 image links\n",
    "hemispheres_links = hemispheres_soup.find_all(\"div\",class_=\"item\")\n",
    "#print(hemispheres_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls  = []\n",
    "for link in hemispheres_links:\n",
    "    #print(link)   ###wl-print the top level info\n",
    "    #print(\"-\"*90)\n",
    "    hemispheres_partial_link = link.find(\"a\",class_=\"itemLink product-item\")\n",
    "    #print(hemispheres_partial_link['href'])  ###wl-print the partial URLs to the 4 images\n",
    "    hemispheres_partial_link = hemispheres_partial_link['href']\n",
    "    full_img_url = hemispheres_main + hemispheres_partial_link\n",
    "    #print(full_img_url)   ###wl-print the full image url for each sub-product\n",
    "    \n",
    "    ##### Open Beautiful Soup on sub-product page #####\n",
    "    sub_product_response = requests.get(full_img_url)\n",
    "    sub_product_soup = BeautifulSoup(sub_product_response.text,'html.parser')\n",
    "    #print(sub_product_soup.prettify())  ###wl-print sub product soup html\n",
    "    \n",
    "    ##### Find sub product image url #####\n",
    "    sub_product_partial_link = sub_product_soup.find_all(\"img\",class_=\"wide-image\")\n",
    "    #print(sub_product_partial_link)   ###wl-comment: some reason this works> These print statements show the decomposition of the 'src'\n",
    "    #print(sub_product_partial_link[0]) ###wl-comment: some reason this works> These print statements show the decomposition of the 'src'\n",
    "    #print(sub_product_partial_link[0]['src']) ###wl-comment: some reason this works> These print statements show the decomposition of the 'src'\n",
    "    \n",
    "    sub_product_full_link = hemispheres_main + sub_product_partial_link[0]['src']\n",
    "    # @@@print(sub_product_full_link)\n",
    "  \n",
    "    ###wl- @TODO: get the title  ###done\n",
    "    ###wl- @TODO: store title and full link in dict\n",
    "   \n",
    "    title = link.find(\"h3\").text\n",
    "    #print(title)\n",
    "    hemisphere_image_urls.append({ \"title\" : title,\"img_url\" : sub_product_full_link})\n",
    "#print(hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
